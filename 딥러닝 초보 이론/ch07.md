# ch 07 이론
## Computer Vision
- 출처: 나무위키
- 기계의 시각에 해당하는 부분을 연구하는 컴퓨터 과학의 최신 연구 분야 중 하나이다
- 공학적인 관점에서, 컴퓨터 비전은 인간의 시각이 할 수 있는 몇 가지 일을 수행하는 자율적인 시스템을 만드는 것을 목표로 한다
- 과학적 관점에서는 컴퓨터 비전은 이미지에서 정보를 추출하는 인공 시스템 관련 이론에 관여한다

### VGGNet
- 기존 네트워크들은 5x5 또는 7x7  conv. layer를 사용
	- layer를 거칠 때마다, feature map의 축소 발생
- 3x3 conv. layer를 반복 사용하여 이를 대체 가능
	- 3x3 2번 -> 5x5 conv. layer
	- 3x3 3번 -> 7x7 conv. layer
- +1 padding을 활용하면 feature map의 크기 유지 가능
- 결과적으로 더 깊은 네트워크 달성 가능
	- 더 적은 weight로 더 큰 capacity를 달성
	- 여러 번 활용하므로 더 좋은 성능을 낸다
- 더 깊지만 더 적은 파라미터를 가진 네트워크
- 가볍고 편리하여 많은 분야에 다양하게 활용됨
- 구현
	1. 3x3 conv. layer + 1 padding
	2. 활성함수 + Batch Normalization
	3. 필요에 따라 1-2번 반복
	4. 2x2 maxpooling (stride로 대체 가능)

### ResNet
- 배경
	- ImageNet 대회가 거듭될 수록, 깊은 네트워크가 우승을 차지함
	- 깊은 네트워크를 학습시키는데 애로사항이 많음
		- 최적화 문제: Training loss가 잘 낮아지지 않음
		- Gradeint vanishing 문제
	- 최적의 깊이가 존재할 텐데, 깊어지면 나머지 identity 함수면 될 것 아닌가? (남은 layer들은 아무것도 안하는 것)
- 레이어가 깊어질 수록 높은 성능(기존에는 레이어가 깊어질 수록 낮은 성능)
- Gradient vanishing 문제를 완벽하게 해결하는 방법
- 현재 제안되는 대부분의 큰 네트워크들은 residual connection을 차용

### 전이 학습(Transfer Learning)
- 배경
	- 각 layer는 feature extraction이 가능할 것
	- 데이터가 다르더라도 이미지를 활용한 task에서는 공통된  feature들이 존재할 것 -> 데이터셋이 유사할 수록 공통 feature들이 더 많이 존재
- 어떤 특정 문제를 풀면서 얻은 지식을 다르지만 연관된 문제에 적용
- 제로베이스에서 시작하는 것보다 훨씬 큰 다르지만 연관된 데이터셋으로 pre-training, fine-tuning 된 모델로 학습하는 것이 더 좋다 -> 큰 데이터셋을 통해 미리 훈련한 네트워크를 나의 문제에 재학습 시키자
- 방법
	1. 전체 다 학습
	2. 불러오지 않은 파트는 freeze, 불러 온 파트만 학습
	3. 불러 온 파트는 학습률을 낮게, 안 불러 온 파트는 학습률 을 높게 설정해서 학습
