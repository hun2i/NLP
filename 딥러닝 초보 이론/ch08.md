# ch 08 이론
## RNN(Recurrent Neural Networks)
- Time Series
	- Time-stamp의 유무에 따른 차이
	- 해당 데이터가 발생한 시각 정보가 매우 중요함
		- 주식 데이터
			- 가격의 순서 및 발생 시점
		- 센서 데이터
- Sequential Data
	- 데이터의 순서 정보가 매우 중요함
		- 텍스트
			- 단어의 순서
		- (샘플링 주기가 일정한) 영상 / 음성
- $x$ 뿐만 아니라 이전 step의 출력을 입력으로 받는다
$$h_{t} = f(x_{t}, h_{t-1}; \theta)$$

### Tensor
- |$x$| = (batch_size, n, input_size)
-  Hidden Tensor = |$h_{t}$| = (batch_size, hidden_size)
- 텐서를 넣으면 텐서가 튀어나온다

### Single layer RNN
- hidden state는 곧 output

### Multi-layered RNN
- 여러개의 층을 쌓아 올린다
- number_of_layers = 층의 수
- Output은 마지막 layer의 모든 time-step의 hidden state이다
- Hidden state는 마지막 time-step의 모든 layer는 hidden state이다

### Bi-directional RNN
- Output은 hidden state가 2배가 된다
- Hidden state는 layer의 갯수가 2배가 된다
- 입력이 한 번에 다 주어지는 경우에 사용 Non-autograd (Many to One, Many to Many)

### 두 가지 접근법
1. Non-autoregressive(Non-generative)
	- 입력이 한 번에 다 들어온다
	- 현재 상태가 앞/뒤 상태를 통해 정해지는 경우
		- Text Classificaiton
	- Bidirection RNN 사용 권장
3. Autogressive(Generative)
	- 현재 상태가 과거 상태에 의존하여 정해지는 경우
		- Natural Language Generation, Machine Translation
	- One-to-Many case 해당
	- Bidirection RNN 사용 불가 -> 미래 timestamp가 아직 주어지지 않음

### BBTT(Back-propagtion Through Time)
- BPTT 알고리즘에 따라, time-step이 많아질수록 gradient의 덧셈이 많아짐
	- 즉, 긴 시퀀스의 경우 gradient가 커질것
	- 시퀀스의 길이에 따라 적절한 learning rate의 크기가 바뀔 수 있음
- 여러 경로로 feed-forward 될 경우, back-propagation 할 때, 최종 gradient 값은 각 경로의 gradient들의 총 합이 된다
- RNN 경우에도 각 time-step에 따라 feed-forward되므로, 각 경로로부터 전달되어 온 gradient들이 더해진다
- 따라서 Vanilla RNN에서는 time-step의 갯수만큼 레이어가 깊어진 것과 마찬가지이므로, tanh를 활성함수로 사용하는 RNN은 gradient vanishing이 발생한다 -> 긴 시퀀스 학습이 어려움 ->  LSTM으로 해결

### LSTM(Long Short Term Memory)
- Cell State: 하나의 또 다른 hidden state로 이해 -> Gradient Vanishing 문제 해결에 결정적인 역할
- LSTM 내부의 gate들이 이전 값의 input과 hidden state를 받아서 흐름을 control(문을 열고 닫는다)
- LSTM vs GRU?
	- 실제로는 LSTM이 좀 더 널리 쓰이는 추세
- vanilla RNN에 비해서 훨씬 많은 파라미터를 가짐
	- 따라서 더 많은 학습 데이터와 학습 시간이 필요
- 비록 Gradient Vanishing 문제를 해결하였지만, 무조건 긴 데이터를 모두 기억할 수 있는 것은 아님
	- Network capacity의 한계는 존재함
	- Attention을 통해 이를 해결할 수 있음
- hidden layer를 깊게 쌓으면 안된다 -> 4개 이상 x

### Gradient Clipping
- 한 번에 많이 갈걸 정해 놓은 threshhold 만큼만 가서 로컬 미니멈에 빠질 확률이 줄어든다 -> learning rate가 아닌 threshhold를 튜닝해서 gradeint exploding을 예방하고 문제를 해결(학습을 조금 더 안정되게)
- Adam optimizer를 사용할 경우 큰 필요가 없음


