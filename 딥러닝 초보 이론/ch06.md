# ch 06 이론
## CNN(Convolutional Neural Networks)
### Convolution Operation
- Convolution filter가 입력($x$)에 대해서 돌아가며 동작
- input($x$) x kernel(k) = output($y$)
- kernel 사이즈가 주어지고 input size의 변화에 따라 output의 사이즈가 변화
- padding: input에 padding을 통해 outputsize를 늘려준다(입력과 출력의 크기를 맞춰준다)
- input channels, output channels 설정

### 입출력 크기 계산 방법
$b$ = batch size
($x_{height}$, $x_{width}$) = input size
$c_{in}$ = # input channels
$c_{out}$ = # output channels
($k_{height}$, $k_{width}$) = kernel size
($y_{height}$, $y_{width}$) = output size

$y$= conv2d($x$),
|$x$| = ($b, c_{in}, x_{height}, x_{width})$
$|y| = (y_{height}, y_{width})$
$= (b, c_{out}, x_{height} - k_{width} + 1, x_{width} - k_{width} + 1)$ 

### Convolution Layer의 특징
- Feature의 위치에 구애 받지 않는다
- 이미지에 특화되어 있다
- 같은 입출력을 갖는 FC Layer에 비해 더 적은 weight를 가진다
- 병렬 계산 구성이 쉬우므로, GPU에서의 연산이 매우 빠르다
- FC Layer에 비해 입출력 크기가 계산이 까다로워, 네트워크 구성이 쉽지 않다 -> 모델 재활용이 쉽지 않음

### CNN 사용 사례
- Computer Vision
- Speech Recognition
- Text Classification
- Machine Translation
- Time Series
- 패턴 인식에서 워낙 월등함을 보여주므로, NLP 등의 다른 분야에도 활발히 사용됨

### Dimension Reduction
- 고차원 공간의 sparse한 데이터를 저차원 공간에 mapping
	- 그 과정에서 복잡하게 얽혀 있는 (entangle) 데이터를 풀어냄
- Max-pooling
	- Down sampling 기법
	- 별도의 max-pooling layer를 활용
	- 초기에 많이 활용됨
	- 4x4 -> 2x2
- Stride
	- 성큼 성큼 걸어가다(한 발자국 -> 두 발자국)
	- 같은 conv layer 내에서 간단히 동작
	- 근래에 좀 더 애용되는 추세
	- 4x4 -> 2x2

### CNN Architecture
- CNN Block
	1. 3x3 Convolution Layer + pad
	2. ReLU
	3. Batch Normalization
	4. 3x3 Convolution Layer + pad (+ with Stride size (2x2))
	5. ReLu
	6. Batch Normalization
	7. (+ Max-pooling if no stride)
		- N: batch size
		- C: channel size
		- H: height
		- W: width
- 가장 쉬운 모델부터 만들자
