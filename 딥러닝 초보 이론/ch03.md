# ch 03 이론
## 확률 분포(함수)
- 입력: 확률 변수 x
-  출력: x가 각 값에 해당 될 때에 대한 확률 값

### 이산 확률 분포(Discrete Probability Distribution)
-  확률 값의 총 합은 1
- 주사위를 던져서 나올 값에 대한 확률의 합은 1
	- (1/6) * 6 = 1
- 예: 단어, 문장

### 연속 확률 분포(Continuous Probability Distribution)
- 확률 밀도 함수(Probability Density Function)
	- 면적의 합이 1
	- 함수 값이 1보다 클 수 있다 -> 넓이만 1이면 되기 때문
- 어떤 샘플이 주어졌을 때, 확률 값을 알 수 없다 -> 구간의 면적이 확률 값이기 때문, 확률 밀도만 알 수 있음
- 예: 그림, 사진, 이미지

### 결합 분포(Joint Probability)
- 두 개 이상의 확률 분포의 결합

### 조건부 확률 분포(Conditional Probability)
- x가 주어졌을 때 y가 나타날 확률(x가 조건)
	- P(y|x) = P(x,y) / P(x)
	- P(x,y) = P(y|x) * P(x)

### 베이즈 정리(Bayes Theorem)
- 데이터 D가 주어졌을 때, 가설 h의 확률
	- P(h|D) = P(D|h) * P(h) / P(D)
	- P(h|D) * P(D) = P(h|D)

### Marginal Distribution
- 결합 분포에서 한 변수를 적분한 형태
$$P(x) =P(x)  \int P(z|x) $$

### 기타
- 확률 값: P($x$)
- 확률 분포 함수: P(x)
	- P($y|x$) = P(Y=$y$|X=$x$)
	- P(y|$x$) = P(Y|X=$x$)

## Likelihood Function
- 입력으로 주어진 확률 분포(파라미터)가 데이터를 얼마나 잘 설명하는지 나타내는 점수(Likelihood)를 출력하는 함수
	- 입력: 확률 분포를 표현하는 파라미터
	- 출력: 데이터를 설명하는 정도
- 데이터를 잘 설명하는지 알 수 있는 방법
	- 데이터가 해당 확률 분포에서 높은 확률 값을 가질 것
- Maximum Likelihood Estimation(MLE): Likelihood 값을 최대로 하는 파라미터를 찾아가는 과정 -> Gradient Ascent를 통해서
- Log Likelihood
	- Likelihood는 확률 값의 곱으로 표현됨 -> Underflow 가능성
	- Log를 취하여 곱셈을 덧셈으로 바꾸고, Log Likelihood로 문제를 해결
		- 덧셈이 곱셈보다 연산도 빠름
	- 대부분의 딥러닝 프레임워크들은 Gradient Descent만 지원
		- Negative Log Likelihood(NLL) : minimization 문제로 접근
		- Gradient Descent를 수행하기 위해선, 파라미터에 대한 미분이 필요함 -> 이를 효율적으로 수행하기 위해 back-propagation을 활용

## MAP (Maximum A Posterior)
### Bayesian 관점
- 파라미터 또한 random variable이며, prior 분포를 따를 것
- 미래의 uncertainty까지 고려
- Prior에 대한 가정이 필요
$$ \hat{\theta } = argmax P(\theta|D) = argmax \frac{P(D|\theta)P(\theta)}{P(D)} = argmax P(D|\theta) P(\theta)$$

### Frequentist 관점
- 파라미터는 최적화의 대상
- 현재까지의 정보를 바탕으로 추정
- Overfitting에 취약함
$$ \hat{\theta} = argmax P(D;\theta) $$

## Information
- 통신이나 압축을 위해 주로 다루어지던 분야
- 불확실성을 나타내는 값
	- 확률이 낮을수록 큰 값(엄청난 정보랭)을 나타낸다
	- 뻔한 값은 정보가 별로 없어도 되기 때문

### Entropy
- 정보량의 기대값(평균)
- 분포의 평균적인 uncertainty를 나타내는 값
	- 분포의 형태를 예측해볼 수 있음
- 엔트로피 H(P)가 클 수록 flatten한 형태, 작을 수록 sharp한 형태

### Cross Entropy
- 분포 P의 관점에서 본 분포 Q의 정보량의 평균
- 두 분포 H(P,Q)가 비슷할 수록 작은 값을 가진다
- Classification에서 Loss를 minimize

### MSE
- Regression에서 Loss를 minimize
