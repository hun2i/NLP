# ch 05 이론
## Boilerplate
- 학습 scheme이 비슷할 경우, 모델과 dataset을 제외한 코드는 거의 동일함
	- 정작 중요한 모델을 코딩하는 시간보다 부수적인 요소 코딩에 더 많은 시간 소요
- 모델과 dataset만 갈아끼워 재사용 가능한 코드가 있으면 좋을 것 -> 나만의 템플릿을 만들자

## pytorch Ignite
- process_function
	- 코드를 깔끔하게 정리할 수 있음

### 코드 풀이
- loss는 float형으로 변환해줘야 메모리 낭비가 없다
- train_loader의 파라미터인 shuffle은 반드시 True로 설정
- test_loader의 파라미터인 shuffle은 일반적으로 False로 설정
- ifinstance(y, torch.LongTensor) == True : Classfication
- p_norm은 파라미터로 학습이 진행됨에 따라 커진다
- g_norm이 크면 모델이 조금만 업데이트 되어도 Loss 값이 변동이 심하다 -> 학습이 진행됨에 따라 줄어든다(0은 아님)
