# ch 04 이론
## 차원의 저주(Curse of Dimensionality)
- 차원이 높을 수록 데이터는 희소하게 분포하게 되어 학습이 어려워진다. (희소성 sparseness 증가, modeling의 난이도가 높아짐)
- 모든 점들을 학습하기 위해서, 모든 구역들을 살펴보아야 함
- 같은 구역 내의 점들은 서로 구별할 수 없음
- 차원이 높아질 수록 빈 공간이 많아진다 -> 모든 구역을 살펴보는게 의미가 없어진다
- 그렇다고 1차원이 좋다고는 할 수 없다 -> 점들간의 구별이 어려워짐 -> 2차원이 이를 어느정도 해결(적절한 차원이 존재)
- 데이터의 특징(feature)을 더럽히지 않으면서 낮은 차원에서 표현해야 함

## 차원 축소(Dimension Reduction)
### Linear Dimension Reduction: PCA
- n차원의 공간에 샘플들의 분포가 주어졌을 때, 분포를 잘 설명하기 위한 새로운 axis를 찾아내는 과정
- 새롭게 찾아낸 axis에  샘플들을 투사(projection)하면 차원 축소가 가능
	- 좋은 axis를 찾는 조건 2가지
		- 선밖의 점을 투사한 선 상의 빨간점 사이의 거리의 합이 최대가 되도록
		- 선 밖의 검은점과 선 사이 거리의 합이 최소가 되도록 -> 정보의 손실이 최소화
- 한계점: 비선형적인 데이터들을 분리하지 못함

### Manifold 가설
- 딥러닝의 차원 축소 기반
- 고차원 공간의 샘플들이 저차원 다양체(manifold)의 형태로 분포해 있다는 가정
	- 다양체를 해당 차원의 공간에 mapping할 수 있음
	- 3차원 정육면체에 2차원의 사각형이 존재
	- 지구라는 3차원 좌표계에 살지만, 2차원 좌표계로 세상을 인식(메르카토르 도법)
- 고차원 공간에서의 두 점 사이의 거리는 저차원 공간으로 맵핑 후 거리와 다르다





